# LLM Architecture Overview

## Current vs New Architecture

### BEFORE (Current State)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      main.py                            â”‚
â”‚                CryptoTradingAgent                       â”‚
â”‚                                                         â”‚
â”‚  __init__():                                           â”‚
â”‚    self.strategy = LLMTradingStrategy(                â”‚
â”‚        aws_region='us-east-1',                        â”‚
â”‚        enable_llm=True                                â”‚
â”‚    )                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ creates
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           src/strategy/llm_strategy.py                  â”‚
â”‚              LLMTradingStrategy                         â”‚
â”‚                                                         â”‚
â”‚  __init__():                                           â”‚
â”‚    # HARDCODED BEDROCK                                â”‚
â”‚    self.bedrock_client = boto3.client(                â”‚
â”‚        'bedrock-runtime',                             â”‚
â”‚        region_name='us-east-1'                        â”‚
â”‚    )                                                  â”‚
â”‚                                                        â”‚
â”‚  analyze_market_sentiment():                          â”‚
â”‚    # DIRECT BOTO3 API CALLS                           â”‚
â”‚    body = json.dumps({...})                           â”‚
â”‚    response = self.bedrock_client.invoke_model(       â”‚
â”‚        modelId='claude-3-5-haiku',                    â”‚
â”‚        body=body                                      â”‚
â”‚    )                                                  â”‚
â”‚    # Parse response manually                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ calls directly
                     â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ AWS Bedrock  â”‚
              â”‚   (boto3)    â”‚
              â”‚ ONLY OPTION  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Problems:**
- ğŸ”´ Cannot use OpenAI without rewriting code
- ğŸ”´ Duplicated LLM logic (sentiment analysis, parsing, caching)
- ğŸ”´ Tight coupling to AWS
- ğŸ”´ Hard to test (requires AWS credentials)
- ğŸ”´ No fallback to other providers

---

### AFTER (New Modular Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      main.py                            â”‚
â”‚                CryptoTradingAgent                       â”‚
â”‚                                                         â”‚
â”‚  __init__():                                           â”‚
â”‚    # Option 1: Auto-create based on env var           â”‚
â”‚    self.strategy = LLMTradingStrategy(                â”‚
â”‚        enable_llm=True                                â”‚
â”‚    )  # Reads LLM_PROVIDER from .env                 â”‚
â”‚                                                        â”‚
â”‚    # Option 2: Dependency injection                   â”‚
â”‚    llm_client = UnifiedLLMClient()                    â”‚
â”‚    self.strategy = LLMTradingStrategy(                â”‚
â”‚        llm_client=llm_client                          â”‚
â”‚    )                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ creates
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           src/strategy/llm_strategy.py                  â”‚
â”‚              LLMTradingStrategy                         â”‚
â”‚                                                         â”‚
â”‚  __init__(llm_client=None):                           â”‚
â”‚    if llm_client:                                     â”‚
â”‚        self.llm_client = llm_client                   â”‚
â”‚    else:                                              â”‚
â”‚        # Auto-create via factory                      â”‚
â”‚        provider = os.getenv('LLM_PROVIDER')           â”‚
â”‚        self.llm_client = LLMFactory.create_llm_client(â”‚
â”‚            provider=provider                          â”‚
â”‚        )                                              â”‚
â”‚                                                        â”‚
â”‚  analyze_market_sentiment():                          â”‚
â”‚    # SIMPLE DELEGATION                                â”‚
â”‚    return self.llm_client.analyze_market_sentiment(   â”‚
â”‚        symbol, news_data                              â”‚
â”‚    )                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ uses
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              src/llm/llm_factory.py                     â”‚
â”‚                 LLMFactory                              â”‚
â”‚                                                         â”‚
â”‚  create_llm_client(provider='openai'):                 â”‚
â”‚    if provider == 'openai':                            â”‚
â”‚        return OpenAILLMClient()                        â”‚
â”‚    elif provider == 'bedrock':                         â”‚
â”‚        return BedrockLLMClient()                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â†“                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAILLMClient  â”‚    â”‚ BedrockLLMClient â”‚
â”‚   (OpenAI)       â”‚    â”‚   (AWS Bedrock)  â”‚
â”‚                  â”‚    â”‚                  â”‚
â”‚ _call_llm():     â”‚    â”‚ _call_llm():     â”‚
â”‚   openai.chat... â”‚    â”‚   boto3.invoke...â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚                       â”‚
    Both inherit from            â”‚
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   BaseLLMClient       â”‚
         â”‚   (Abstract Base)     â”‚
         â”‚                       â”‚
         â”‚ - Caching logic       â”‚
         â”‚ - Prompt templates    â”‚
         â”‚ - Response parsing    â”‚
         â”‚ - Validation          â”‚
         â”‚ - Fallback handling   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Switch providers via environment variable
- âœ… Shared logic in base class (DRY)
- âœ… Easy to add new providers
- âœ… Better testability (mock LLM client)
- âœ… Auto-fallback between providers

---

## Unified Client with Auto-Fallback

For maximum reliability, use `UnifiedLLMClient`:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                UnifiedLLMClient                         â”‚
â”‚           (Automatic Failover)                          â”‚
â”‚                                                         â”‚
â”‚  __init__(auto_fallback=True):                         â”‚
â”‚    self.primary = OpenAILLMClient()                    â”‚
â”‚    self.fallback = BedrockLLMClient()                  â”‚
â”‚                                                         â”‚
â”‚  analyze_market_sentiment():                           â”‚
â”‚    try:                                                â”‚
â”‚        return self.primary.analyze_market_sentiment()  â”‚
â”‚    except Exception:                                   â”‚
â”‚        return self.fallback.analyze_market_sentiment() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
         â”‚ Primary                      â”‚ Fallback
         â†“                              â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  OpenAI     â”‚              â”‚ AWS Bedrock â”‚
  â”‚  GPT-4o-miniâ”‚              â”‚ Claude 3.5  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

If OpenAI fails â†’ Automatically uses Bedrock
```

---

## Data Flow Through the System

### 1. User Starts Analysis

```
User runs: python main.py
           â”‚
           â†“
    CryptoTradingAgent.__init__()
           â”‚
           â”œâ”€ Reads LLM_PROVIDER from .env
           â”‚  (bedrock or openai)
           â”‚
           â”œâ”€ Creates LLM client via factory
           â”‚
           â””â”€ Injects into LLMTradingStrategy
```

### 2. Analysis Cycle Begins

```
main.py: run_analysis_cycle()
    â”‚
    â”œâ”€ Every TRADING_INTERVAL_MINUTES (default: 15 min)
    â”‚
    â””â”€ For each symbol in TRADING_PAIRS:
           â”‚
           â””â”€ analyze_symbol(symbol)
```

### 3. Symbol Analysis Pipeline

```
analyze_symbol("BTC-USDT")
    â”‚
    â”œâ”€ [1] Fetch Market Data
    â”‚      market_data.get_historical_klines()
    â”‚      â†’ Returns 500 candles (OHLCV data)
    â”‚
    â”œâ”€ [2] Fetch News Data (if LLM enabled)
    â”‚      market_data.get_news_data()
    â”‚      â†’ Returns recent news articles
    â”‚
    â”œâ”€ [3] Generate Trading Strategy
    â”‚      strategy.generate_trading_strategy(symbol, df, news)
    â”‚      â”‚
    â”‚      â”œâ”€ A. Technical Analysis (Always)
    â”‚      â”‚    calculate_technical_indicators(df)
    â”‚      â”‚    â†’ RSI, MACD, BB, Stochastic, etc.
    â”‚      â”‚    â†’ 15+ indicators calculated
    â”‚      â”‚
    â”‚      â”œâ”€ B. Market Regime Detection
    â”‚      â”‚    detect_market_regime(df)
    â”‚      â”‚    â†’ Bull/Bear/Sideways/Volatile
    â”‚      â”‚
    â”‚      â”œâ”€ C. Technical Signals
    â”‚      â”‚    calculate_technical_signals(df)
    â”‚      â”‚    â†’ Buy/Sell/Hold scores
    â”‚      â”‚
    â”‚      â”œâ”€ D. LLM Sentiment (Optional) ğŸ¤–
    â”‚      â”‚    IF use_llm AND news_data:
    â”‚      â”‚        llm_client.analyze_market_sentiment()
    â”‚      â”‚        â”‚
    â”‚      â”‚        â”œâ”€ Check cache (15 min TTL)
    â”‚      â”‚        â”‚  IF cached â†’ return cached result
    â”‚      â”‚        â”‚
    â”‚      â”‚        â”œâ”€ Create prompt from news
    â”‚      â”‚        â”‚
    â”‚      â”‚        â”œâ”€ Call LLM API
    â”‚      â”‚        â”‚  (OpenAI or Bedrock based on config)
    â”‚      â”‚        â”‚
    â”‚      â”‚        â”œâ”€ Parse JSON response
    â”‚      â”‚        â”‚  {
    â”‚      â”‚        â”‚    sentiment_score: 0.7,
    â”‚      â”‚        â”‚    confidence: 0.85,
    â”‚      â”‚        â”‚    recommendation: "buy"
    â”‚      â”‚        â”‚  }
    â”‚      â”‚        â”‚
    â”‚      â”‚        â””â”€ Cache result
    â”‚      â”‚
    â”‚      â””â”€ E. Combine Signals
    â”‚           _combine_signals(technical, llm_sentiment)
    â”‚           â†’ Weighted combination
    â”‚           â†’ Final decision: BUY/SELL/HOLD
    â”‚
    â”œâ”€ [4] Portfolio-Aware Position Sizing
    â”‚      portfolio_manager.get_position_size_recommendation()
    â”‚      â†’ Considers current holdings
    â”‚      â†’ Calculates safe position size
    â”‚
    â”œâ”€ [5] Risk Validation
    â”‚      risk_manager.validate_trade()
    â”‚      â†’ Check exposure limits
    â”‚      â†’ Verify stop-loss levels
    â”‚
    â””â”€ [6] Execute Trade (if approved)
           execute_trade()
           â†’ Place order on KuCoin
           â†’ Record in trade tracker
```

---

## Configuration Matrix

| Scenario | LLM_PROVIDER | FAST_MODE | Result |
|----------|--------------|-----------|---------|
| Fast Trading | - | true | Technical only, 2-3s |
| OpenAI Analysis | openai | false | Technical + OpenAI, 5-8s |
| Bedrock Analysis | bedrock | false | Technical + Bedrock, 10-15s |
| Auto-Fallback | openai | false | OpenAI â†’ Bedrock on fail |
| No LLM Available | - | false | Technical only (fallback) |

---

## Example: Complete Request Flow

```
1. User Input
   export LLM_PROVIDER=openai
   export OPENAI_API_KEY=sk-...
   python main.py

2. Initialization
   CryptoTradingAgent()
     â””â”€ LLMTradingStrategy()
          â””â”€ LLMFactory.create_llm_client('openai')
               â””â”€ OpenAILLMClient()
                    â””â”€ Tests connection âœ“

3. Analysis Cycle (every 15 min)
   For BTC-USDT:

   [Market Data] â”€â”€â”€â”€â”€â”€â”
   500 candles         â”‚
   OHLCV data          â”‚
                       â”œâ”€â”€â†’ Technical Indicators
   [News Data] â”€â”€â”€â”€â”€â”€â”€â”€â”¤    RSI: 35 (oversold)
   "BTC breaks $100k"  â”‚    MACD: bullish cross
   "Institutions buy"  â”‚    BB: near lower band
                       â”‚
                       â”œâ”€â”€â†’ LLM Sentiment Analysis
                       â”‚    OpenAI GPT-4o-mini:
                       â”‚    {
                       â”‚      sentiment: 0.8 (bullish)
                       â”‚      confidence: 0.9
                       â”‚      recommendation: "buy"
                       â”‚    }
                       â”‚
                       â””â”€â”€â†’ Combined Signal
                            Technical: 0.7
                            LLM: 0.8
                            Final: 0.75 â†’ BUY
                            Confidence: 85%

4. Position Sizing
   Portfolio: $10,000
   Current BTC: 0
   Recommendation: Buy $1,500 (15%)

5. Risk Check
   Max position: 25% âœ“
   Available funds: $8,000 âœ“
   Trade approved âœ“

6. Execution
   Order: Buy 0.015 BTC @ $100,000
   Total: $1,500
   Status: Executed âœ“

7. Tracking
   Record trade in JSON
   Update portfolio
   Log P&L
```

---

## Summary

### What This Is
This is a **CLI trading agent** (not a web app) that:
- Runs continuously in Docker or interactively
- Analyzes crypto markets every 15 minutes
- Uses LLM for sentiment analysis
- Executes trades on KuCoin exchange

### How LLM Integrates
The LLM provides **sentiment analysis** that is:
- Combined with technical indicators
- Used to make buy/sell decisions
- Optional (can be disabled with FAST_MODE)
- Cached to reduce costs
- Provider-agnostic (Bedrock or OpenAI)

### New Modular Design
The new LLM clients:
- âœ… Abstract away provider differences
- âœ… Enable easy switching between OpenAI/Bedrock
- âœ… Provide automatic failover
- âœ… Reduce code duplication
- âœ… Improve testability

To fully integrate, we need to **refactor `LLMTradingStrategy`** to use the new clients instead of direct boto3 calls.
